The complexity of a \ac{LBA} increases with the complexity of the system it has
to work with.
Consider a load balancing algorithm that computes the target worker for a task,
where the size of the tasks are equal, the workers have the same capabilities
and the number of overall tasks is known.
Such an algorithm is not of complexity.
In fact, it can be implemented by a simple mathematical equation:
\begin{equation}
    i = j \% |W|
\end{equation}
Where $i$ is the number of the worker instance in set $W$ that will be assigned
with the task number $j$.
Though, such an environment is unlikely.

In todays systems, the size of a task cannot be predicted.
The capabilities of a worker instance may be known, but as tasks come in and are
assigned to workers, an algorithm has to implement some book-keeping to know
which worker has the least load at a given point in time.
This is not complex, though having only one load balancer in a large ecosystem
is highly unlikely, as it would be a single point of failure.
Instead, multiple load balancers are used.
Because of this, some synchronization method between the book-keeping of each
load balancer must be introduced to ensure the best distribution of tasks.

If worker instance $W_1$ has the least load at time $t_1$, the load balancer
$L_1$ might assign a number of tasks to it in $t_2$.
After that, $L_1$ must notify $L_2$ about the new load on $W_1$, which it does
in $t_3$.
In the meantime, load balancer $L_2$ might need to assign another set of tasks,
which it assignes to $W_1$ in $t_2$, because it does not know yet that $W_1$
isn't the least loaded worker anymore.
This causes $W_1$ to become overloaded in $t_3$.

\subsection{Random load balancing}

A cheap load balancing strategy is randomizing the target.
\begin{equation}
    i = rand() \% |W|
\end{equation}
Which has several advantages over other approaches:
\begin{itemize}
    \item Simplicity
    \item Few edge cases
    \item Easy failover
    \item Works distributed
\end{itemize} % TODO: cite http://www.codemesh.io/codemesh/tyler-mcmullen ?
Although, distributing load randomly does not guarantee the best distributing
over a given set of notes.
It is even possible that a small set of workers are assigned
with a huge number of tasks while other workers only handle a very small number
of tasks, resulting in a high load on some machines while others idle.

The desireable state is that each machine has the same workload, resulting in a
perfect distribution of load over all machines (blue in \ref{fig:algo:random}).
With an algorithm randomly assigning work to workers, the possibility exists
that the system ends up in an unbalanced state as shown in \ref{fig:algo:random}
(red).
Though this is very unlikely, we can do better.

% TODO: Fix numbering
\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			enlargelimits=0.15,
			ylabel={load (percent)},
			symbolic x coords={1,2,3,4,5},
			xtick=data,
			nodes near coords align={vertical},
			]
			\addplot+[color=blue]
				coordinates {(1,100) (2,100) (3,100) (4,100) (5,100)};

			\addplot+[color=red]
				coordinates {(1,47 ) (2,74 ) (3,170) (4,54 ) (5,155)};

			\addplot+[color=green]
				coordinates {(1,103) (2,107) (3,97 ) (4,100) (5,92 )};
		\end{axis}
	\end{tikzpicture}
	\label{fig:algo:random}
	\caption{Example: Resulting load from random task distribution}
\end{figure}

\subsection{\ac{JSQ}}

Another known load balancing algorithm is \ac{JSQ}.
With this algorithm, the load balancing mechanism implements some book-keeping
mechanism how many jobs are in the queues of the worker instances and selects
the instance with the shortest queue for a new task.
This algorithm can result in a ``herd effect'', where a subset of machines is
either underutilized or overloaded with tasks.
That is likely, if the load balancing instance(s) has old information on the
utilization of the nodes as described in \cite{inpSLoadInfo}.

An example result for \ac{JSQ} is visualized in \ref{fig:algo:random} (green)
using 500 tasks with random requirements.

A surprising optimization is the combination of the two algorithms described
above.
\begin{equation}
    \begin{aligned}
        x &= rand() \% |I| \\
        y &= rand() \% |I| \\
        i &= \begin{cases}
            x & \text{if } load(W_x) =< load(W_y)\\
            y & \text{if } load(W_x) > load(W_y)\\
        \end{cases}
    \end{aligned}
    \label{eq:randjsq}
\end{equation}
If we combine these algorithms, so we select two worker instances randomly and
now apply \ac{JSQ} on this new set of worker instances (see \ref{eq:randjsq}),
we get better distribution, as stated in \cite{powerOfTwoInRLB}.

% vim: set ts=4 sw=4 tw=0 noet :
